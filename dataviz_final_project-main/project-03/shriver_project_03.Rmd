---
title: "Visualizing Text and Distributions"
output: 
  html_document:
    keep_md: true
    toc: true
    toc_float: true
---

# Data Visualization Project 03


## Visualizing Text Data

Review the set of slides (and additional resources linked in it) for visualizing text data: https://www.reisanar.com/slides/text-viz#1

Choose any dataset with text data, and create at least one visualization with it. For example, you can create a frequency count of most used bigrams, a sentiment analysis of the text data, a network visualization of terms commonly used together, and/or a visualization of a topic modeling approach to the problem of identifying words/documents associated to different topics in the text data you decide to use. 

Make sure to include a copy of the dataset in the `data/` folder, and reference your sources if different from the ones listed below:

- [Billboard Top 100 Lyrics](https://github.com/reisanar/datasets/blob/master/BB_top100_2015.csv)

- [RateMyProfessors comments](https://github.com/reisanar/datasets/blob/master/rmp_wit_comments.csv)

- [FL Poly News 2020](https://github.com/reisanar/datasets/blob/master/poly_news_FL20.csv)

- [FL Poly News 2019](https://github.com/reisanar/datasets/blob/master/poly_news_FL19.csv)

(to get the "raw" data from any of the links listed above, simply click on the `raw` button of the GitHub page and copy the URL to be able to read it in your computer using the `read_csv()` function)

#### I will be working on a visualization of frequent word usage in RateMyProfessor comments

Saving the csv to a dataframe

```{r}
library(tidyverse)
library(tidytext)
ratings <- read.csv("https://raw.githubusercontent.com/reisanar/datasets/master/rmp_wit_comments.csv")
ratings
```

Need to get comments into one-term-per-row format, also would like to remove stop words as well as various terms within that may not be very meaningful

```{r}
# creating dataframe of extra words to remove
remove_words <- data.frame(word = c("professor", "class", "student", "material", "lectures", "homework", "math", "calc", "pre", "lecture", "semester", "found", "understand"))

ratings_words <- ratings %>%
  # unnesting comments into words
  unnest_tokens(output = word, input = comments) %>%
  
  # removing stop words and previously listed words
  anti_join(stop_words, by = "word") %>%
  anti_join(remove_words, by = "word") %>%
  
  # making the wordcounts
  group_by(course) %>%
  count(word, sort = TRUE) %>%
  top_n(9, n) %>%
  ungroup() %>%
  mutate(word = fct_inorder(word))

head(ratings_words)
```

Making a grid of bar charts showing word frequency per course

```{r}
ggplot(ratings_words, aes(x = n, y = fct_rev(word), fill = course)) +
  geom_col() +
  guides(fill = FALSE) +
  labs(x = NULL, y = NULL) +
  scale_fill_viridis_d() +
  facet_wrap(vars(course), scales = "free_y") +
  theme_minimal()
```

